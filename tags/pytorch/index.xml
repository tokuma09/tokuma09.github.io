<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>pytorch on</title><link>https://tokuma09.github.io/tags/pytorch/</link><description>Recent content in pytorch on</description><generator>Hugo -- gohugo.io</generator><language>ja</language><lastBuildDate>Tue, 02 May 2023 00:04:42 +0000</lastBuildDate><atom:link href="https://tokuma09.github.io/tags/pytorch/index.xml" rel="self" type="application/rss+xml"/><item><title>Diffusion Modelサマリー</title><link>https://tokuma09.github.io/blog/20230502/</link><pubDate>Tue, 02 May 2023 00:04:42 +0000</pubDate><guid>https://tokuma09.github.io/blog/20230502/</guid><description>1. はじめに 2020年にDiffusion Modelに関する論文が発表されて以降、生成モデルの研究が進展している。 2021年以降はこのDiffusion Modelをベースとした画像生成AIサービスがリリースされて世間からの注目を集めている。
DALL·E2 Midjourney Stable Diffusion 2023年5月現在生成モデルとして最も注目を集めているのは大規模言語モデル(例: OpenAIのGPT-4)であるが、Diffusion Modelは主に画像生成の文脈で利用されており、モデルのスコープが異なることに注意されたい。
最近のバズワードの大規模言語モデルではないが、生成モデルという枠組みの中では非常に重要かつモデル構造も面白いので、今回はDiffusion Modelを取り扱っていきたい。
TransformerベースにGPTモデルファミリーまで一気に論文読む&amp;amp;実装する会も将来的に扱うつもり。 2. Diffusion Model 早わかり Diffusion Modelは生成モデルの1種であり、forward processとreverse processの２つの確率過程から構成される。
平たく言えば下記の図のようになる。
forward processは画像に僅かなノイズを少しずつ加えるプロセスを示している。 reverse processはノイズになったデータから徐々に元の画像に復元するプロセスを示している。 このreverse processをモデル化してデータから学習する 実際に学習したモデルを用いて、画像生成する場合には何らかのノイズをモデルに与えて、reverse processを経由することで、画像が生成される。Ho et al.(2020)は下記のような生成結果を提示している。
なお、最近の研究ではテキストを入力すると対応する画像が生成されるモデルも存在している。
例: OpenAIのGLIDE このような生成は条件付き生成と呼ばれ、実務上求められるのはこちらの生成方法だろう。
なぜこのようなシンプルな構造でうまくいくのか？については現在も明確な答えはないが、最もシンプルな仮説は下記である。
reverse processが多くの確率層からなる深いネットワークであることが複雑な画像生成を実現している 初期の画像処理モデルも層を深くすることが出来る様になってブレイクスルーが起きた(例: ResNet) 生成プロセス(reverse process)のみをモデル化している VAEは生成モデルと認識モデルを同時に学習 GANは生成モデルと認識モデルを交互に学習 それでは以降では具体的なモデルの中身を見ていきたい。
3. Diffusion Model モデル詳細 本章では岡野原(2023)に基づいてDiffusion Modelの全体像・学習・推論を見ていく。
余談: Ho et al.(2020)を見ていってもよいが、いきなりこれを見ると挫折する。やや遠回りに見えるがLuo et al.(2022)から読み進めていくことを勧める。 3.1 全体像 2で述べたようにDiffusion Modelはforward processとreverse processから構成される。
forward process
forward processはもともとのデータ $\bold{x}_0$ から徐々にデータの情報を減らしてノイズを加えた、ノイズ付与データの系列 $\bold{x}_1, \cdots, \bold{x}_T$を得るマルコフプロセスである。</description></item></channel></rss>